{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dNCbnwQPfrX",
        "outputId": "23a2722a-e0fd-4051-8d5b-4a1da69d039d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVtD9aGgOdgH",
        "outputId": "11a3a9f7-33d9-4a4b-c870-d6efa4a0aef6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 84 (1695.3 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cernbox.cern.ch/remote.php/dav/public-files/e3pqxcIznqdYyRv/Dataset_Specific_Unlabelled.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UljWkl0XTvqa",
        "outputId": "de7e921f-d46c-4769-d7f4-a87b2c7f713d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-30 02:38:39--  https://cernbox.cern.ch/remote.php/dav/public-files/e3pqxcIznqdYyRv/Dataset_Specific_Unlabelled.h5\n",
            "Resolving cernbox.cern.ch (cernbox.cern.ch)... 137.138.120.151, 128.142.53.28, 128.142.53.35, ...\n",
            "Connecting to cernbox.cern.ch (cernbox.cern.ch)|137.138.120.151|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30000002048 (28G) [application/octet-stream]\n",
            "Saving to: ‘Dataset_Specific_Unlabelled.h5’\n",
            "\n",
            "Dataset_Specific_Un 100%[===================>]  27.94G  18.1MB/s    in 26m 38s \n",
            "\n",
            "2025-03-30 03:05:18 (17.9 MB/s) - ‘Dataset_Specific_Unlabelled.h5’ saved [30000002048/30000002048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.optim import SGD,Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import h5py\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "b_GM89XtwYrt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_h5_file(file_path):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        print(f\"Top-level keys: {list(f.keys())}\")\n",
        "\n",
        "        def explore_group(group, prefix=\"\"):\n",
        "            for key in group.keys():\n",
        "                item = group[key]\n",
        "                path = f\"{prefix}/{key}\" if prefix else key\n",
        "\n",
        "                if isinstance(item, h5py.Group):\n",
        "                    print(f\"GROUP: {path}\")\n",
        "                    explore_group(item, path)\n",
        "                elif isinstance(item, h5py.Dataset):\n",
        "                    shape = item.shape\n",
        "                    dtype = item.dtype\n",
        "\n",
        "                    print(f\"DATASET: {path}\")\n",
        "                    print(f\"  Shape: {shape}\")\n",
        "                    print(f\"  Data type: {dtype}\")\n",
        "\n",
        "\n",
        "                    try:\n",
        "                        sample = item[0]\n",
        "                        if isinstance(sample, np.ndarray):\n",
        "                            print(f\"  Sample shape: {sample.shape}\")\n",
        "                            print(f\"  Sample min/max: {sample.min()}/{sample.max()}\")\n",
        "                        else:\n",
        "                            print(f\"  Sample value: {sample}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Error sampling data: {e}\")\n",
        "\n",
        "                    if len(item.attrs) > 0:\n",
        "                        print(f\"  Attributes: {list(item.attrs.keys())}\")\n",
        "\n",
        "                    print(\"-\" * 40)\n",
        "\n",
        "        explore_group(f)\n",
        "\n",
        "\n",
        "explore_h5_file('Dataset_Specific_labelled.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyuPrQWGxfzw",
        "outputId": "c5ee083c-1532-4d41-ed9c-273db6ca2cac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-level keys: ['Y', 'jet']\n",
            "DATASET: Y\n",
            "  Shape: (10000, 1)\n",
            "  Data type: float32\n",
            "  Sample shape: (1,)\n",
            "  Sample min/max: 1.0/1.0\n",
            "----------------------------------------\n",
            "DATASET: jet\n",
            "  Shape: (10000, 125, 125, 8)\n",
            "  Data type: float32\n",
            "  Sample shape: (125, 125, 8)\n",
            "  Sample min/max: 0.0/255.0\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.multiprocessing as mp\n",
        "mp.set_start_method('fork', force=True)"
      ],
      "metadata": {
        "id": "nGaWs94CSxau"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class H5Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, transform=None):\n",
        "        self.file_path = file_path\n",
        "        self.transform = transform\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            self.data_key = 'jet'\n",
        "            self.length = len(f[self.data_key])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        with h5py.File(self.file_path, 'r') as f:\n",
        "            data = f[self.data_key][index]\n",
        "\n",
        "        if data.shape[2] == 8:\n",
        "            rgb_data = data[:, :, :3]\n",
        "        else:\n",
        "            rgb_data = data\n",
        "\n",
        "        if rgb_data.max() > 1.0:\n",
        "            rgb_data = rgb_data / 255.0\n",
        "\n",
        "        if self.transform:\n",
        "            if isinstance(self.transform, SimCLRDataTransform):\n",
        "                view1 = self.transform(rgb_data)\n",
        "                view2 = self.transform(rgb_data)\n",
        "                return (view1, view2), -1\n",
        "            else:\n",
        "                transformed_data = self.transform(rgb_data)\n",
        "                return transformed_data, -1\n",
        "\n",
        "        tensor_data = torch.tensor(rgb_data, dtype=torch.float32).permute(2, 0, 1)\n",
        "        return tensor_data, -1"
      ],
      "metadata": {
        "id": "RoA09jGEVb1E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BbQhSN4SDWSj"
      },
      "outputs": [],
      "source": [
        "class Resnet18Backbone(nn.Module):\n",
        "    def __init__(self, num_classes=1000, backbone_only=False):\n",
        "        super(Resnet18Backbone, self).__init__()\n",
        "        self.backbone_only = backbone_only\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if not backbone_only:\n",
        "            self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(self._block(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(self._block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _block(self, in_channels, out_channels, stride=1):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if not self.backbone_only:\n",
        "            x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(nn.Module):\n",
        "  def __init__(self, feature_dim=128):\n",
        "    super(SimCLR, self).__init__()\n",
        "\n",
        "    self.encoder = Resnet18Backbone(backbone_only=True)\n",
        "    self.projection_head = nn.Sequential(\n",
        "    nn.Linear(512, 1024),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(1024, feature_dim)\n",
        "  )\n",
        "\n",
        "  def forward(self,x):\n",
        "    h = self.encoder(x)\n",
        "    z = self.projection_head(h)\n",
        "    return F.normalize(z, dim=1)\n",
        "\n",
        "  def get_encoder(self):\n",
        "    return self.encoder"
      ],
      "metadata": {
        "id": "uhryZIdDDnSw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class SimCLRDataTransform:\n",
        "#     def __init__(self, size=224):\n",
        "#         # self.transform = transforms.Compose([\n",
        "#         #     transforms.RandomResizedCrop(size=size),\n",
        "#         #     transforms.RandomHorizontalFlip(),\n",
        "#         #     transforms.RandomApply([\n",
        "#         #         transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "#         #     ], p=0.8),\n",
        "#         #     transforms.RandomGrayscale(p=0.2),\n",
        "#         #     transforms.GaussianBlur(kernel_size=int(0.1 * size)),\n",
        "#         #     transforms.ToTensor(),\n",
        "#         #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#         # ])\n",
        "#         self.transform = transforms.Compose([\n",
        "#             transforms.RandomResizedCrop(size=224, scale=(0.2, 1.0)),\n",
        "#             transforms.RandomHorizontalFlip(p=0.5),\n",
        "#             transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
        "#             transforms.RandomGrayscale(p=0.2),\n",
        "#             transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#         ])\n",
        "#     def __call__(self, x):\n",
        "#       return self.transform(x), self.transform(x)\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "class SimCLRDataTransform:\n",
        "    def __init__(self, size=224):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomResizedCrop(size=size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomApply([\n",
        "                transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "            ], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.transform(x), self.transform(x)"
      ],
      "metadata": {
        "id": "v03UFpA9O_E8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.15):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def _get_mask(self, batch_size):\n",
        "        mask = torch.zeros((2 * batch_size, 2 * batch_size))\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 1\n",
        "            mask[batch_size + i, i] = 1\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        batch_size = z_i.shape[0]\n",
        "\n",
        "        mask = self._get_mask(batch_size).to(z_i.device)\n",
        "\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "\n",
        "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
        "        sim_i_j = torch.diag(similarity_matrix, batch_size)\n",
        "        sim_j_i = torch.diag(similarity_matrix, -batch_size)\n",
        "\n",
        "        positive_pairs = torch.cat([sim_i_j, sim_j_i], dim=0)\n",
        "\n",
        "        labels = torch.zeros(2 * batch_size).to(positive_pairs.device).long()\n",
        "\n",
        "        similarity_matrix = similarity_matrix * mask\n",
        "\n",
        "        logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)\n",
        "        logits = similarity_matrix - logits_max.detach()\n",
        "\n",
        "        exp_logits = torch.exp(logits)\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
        "\n",
        "        loss = -mean_log_prob_pos.mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "q9BV4RYAJzx-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# def get_lr(epoch, warmup_epochs=7, max_epochs=35, initial_lr=1e-4, base_lr=1e-3):\n",
        "#     if epoch < warmup_epochs:\n",
        "#         return initial_lr + (base_lr - initial_lr) * epoch / warmup_epochs\n",
        "#     else:\n",
        "#         return base_lr * 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
        "def get_lr(epoch, warmup_epochs=5, max_epochs=35, initial_lr=1e-5, base_lr=5e-3, min_lr=1e-4):\n",
        "    if epoch < warmup_epochs:\n",
        "\n",
        "        return initial_lr + (base_lr - initial_lr) * epoch / warmup_epochs\n",
        "    else:\n",
        "\n",
        "        cosine_factor = 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
        "        return min_lr + (base_lr - min_lr) * cosine_factor\n"
      ],
      "metadata": {
        "id": "i0m5qkGQW9o3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "def train_simclr(model, data_loader, optimizer, epochs=100, device='cuda'):\n",
        "    criterion = NTXentLoss(temperature=0.20)\n",
        "    model = model.to(device)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        current_lr = get_lr(epoch, warmup_epochs=5, max_epochs=epochs, initial_lr=1e-5, base_lr=5e-3)\n",
        "        for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = current_lr\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        for batch in data_loader:\n",
        "            views, _ = batch\n",
        "\n",
        "\n",
        "            # x_i = torch.stack([view[0] for view in views]).to(device)\n",
        "            # x_j = torch.stack([view[1] for view in views]).to(device)\n",
        "\n",
        "            # z_i = model(x_i)\n",
        "            # z_j = model(x_j)\n",
        "            x_i = torch.stack([view[0] for view in views])\n",
        "            x_j = torch.stack([view[1] for view in views])\n",
        "\n",
        "            x_i = x_i.view(-1, 3, 224, 224).to(device)\n",
        "            x_j = x_j.view(-1, 3, 224, 224).to(device)\n",
        "\n",
        "\n",
        "            with autocast():\n",
        "                z_i = model(x_i)\n",
        "                z_j = model(x_j)\n",
        "                loss = criterion(z_i, z_j)\n",
        "\n",
        "\n",
        "            # loss = criterion(z_i, z_j)\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "        #     optimizer.zero_grad()\n",
        "        #     loss.backward()\n",
        "        #     optimizer.step()\n",
        "\n",
        "        #     total_loss += loss.item()\n",
        "\n",
        "\n",
        "        # scheduler.step()\n",
        "\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, f'simclr_resnet15_epoch_{epoch+1}.pt')\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), 'simclr_resnet15_final.pt')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "FVHn-EQJRc2z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, encoder, num_classes, feature_dim=1000):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        features = self.encoder(x)\n",
        "        return self.classifier(features)"
      ],
      "metadata": {
        "id": "2hAkT1SDRv2d"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, encoder, output_dim=1, feature_dim=1000):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return self.regressor(features)"
      ],
      "metadata": {
        "id": "b0_Suh9JVZVX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SimCLRDataTransform:\n",
        "#     def __init__(self, size=224):\n",
        "#         self.transform = transforms.Compose([\n",
        "#             transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
        "#             transforms.Resize(size),  # Resize to desired size\n",
        "#             transforms.RandomResizedCrop(size=size),\n",
        "#             transforms.RandomHorizontalFlip(),\n",
        "#             transforms.RandomApply([\n",
        "#                 transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "#             ], p=0.8),\n",
        "#             transforms.RandomGrayscale(p=0.2),\n",
        "#             transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#         ])\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         return self.transform(x)"
      ],
      "metadata": {
        "id": "rEm7_Yz0xD7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "#     # Initialize model\n",
        "#     model = SimCLR(feature_dim=128)\n",
        "\n",
        "#     # Setup data loading\n",
        "#     transform = SimCLRDataTransform(size=224)\n",
        "#     dataset = H5Dataset('Dataset_Specific_Unlabelled.h5', transform=transform)\n",
        "\n",
        "#     # Start with num_workers=0 for debugging\n",
        "#     data_loader = DataLoader(\n",
        "#         dataset,\n",
        "#         batch_size=32,  # Smaller batch size for testing\n",
        "#         shuffle=True,\n",
        "#         num_workers=0,  # No multiprocessing for initial testing\n",
        "#         pin_memory=True\n",
        "#     )\n",
        "\n",
        "#     # Try loading a single batch to test\n",
        "#     try:\n",
        "#         sample_batch = next(iter(data_loader))\n",
        "#         print(f\"Successfully loaded a batch: {type(sample_batch)}\")\n",
        "\n",
        "#         if isinstance(sample_batch, list):\n",
        "#             print(\"Batch is a list. Inspecting its contents:\")\n",
        "#             for i, item in enumerate(sample_batch):\n",
        "#                 if isinstance(item, list):\n",
        "#                     print(f\"Item {i} is a list with {len(item)} elements\")\n",
        "#                     # Inspect all elements of this nested list\n",
        "#                     for j, nested_item in enumerate(item):\n",
        "#                         print(f\"  Element {j} type: {type(nested_item)}\")\n",
        "#                         if hasattr(nested_item, 'shape'):\n",
        "#                             print(f\"  Element {j} shape: {nested_item.shape}\")\n",
        "#                         else:\n",
        "#                             print(f\"  Element {j} has no shape attribute\")\n",
        "#                 elif hasattr(item, 'shape'):\n",
        "#                     print(f\"Item {i}: Type: {type(item)}, Shape: {item.shape}\")\n",
        "#                 else:\n",
        "#                     print(f\"Item {i}: Type: {type(item)}, No shape attribute\")\n",
        "#         else:\n",
        "#             print(\"Batch is not a list. Unexpected format.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error inspecting batch: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ph35iXzKwrkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "    model = SimCLR(feature_dim=128)\n",
        "    model = model.to(device)\n",
        "\n",
        "    transform = SimCLRDataTransform(size=224)\n",
        "    unlabeled_dataset = H5Dataset('Dataset_Specific_Unlabelled.h5', transform=transform)\n",
        "\n",
        "    batch_size = 128\n",
        "    unlabeled_loader = DataLoader(\n",
        "        unlabeled_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "        num_workers=6,\n",
        "        prefetch_factor=4\n",
        "    )\n",
        "\n",
        "    print(f\"Unlabeled dataset size: {len(unlabeled_dataset)}\")\n",
        "    print(f\"Number of batches: {len(unlabeled_loader)}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        sample_batch = next(iter(unlabeled_loader))\n",
        "        print(f\"Batch structure verification:\")\n",
        "        print(f\"  Batch type: {type(sample_batch)}\")\n",
        "\n",
        "        views, labels = sample_batch\n",
        "        print(f\"  Views type: {type(views)}, Labels shape: {labels.shape}\")\n",
        "\n",
        "        if isinstance(views, list):\n",
        "\n",
        "            if isinstance(views[0], torch.Tensor):\n",
        "                print(f\"  First view shape: {views[0].shape}\")\n",
        "                if len(views) > 1:\n",
        "                    print(f\"  Second view shape: {views[1].shape}\")\n",
        "\n",
        "            elif isinstance(views[0], list):\n",
        "                print(f\"  Views[0] is a list with {len(views[0])} elements\")\n",
        "                if isinstance(views[0][0], torch.Tensor):\n",
        "                    print(f\"  First view shape: {views[0][0].shape}\")\n",
        "                    if len(views[0]) > 1:\n",
        "                        print(f\"  Second view shape: {views[0][1].shape}\")\n",
        "\n",
        "        print(\"Batch structure looks correct for SimCLR training\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error verifying batch structure: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.06, momentum=0.9, weight_decay=5e-4)\n",
        "    # from torchlars import LARS\n",
        "    # optimizer = LARS(torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9))\n",
        "\n",
        "    print(\"Starting SimCLR pretraining...\")\n",
        "    pretrained_model = train_simclr(\n",
        "        model=model,\n",
        "        data_loader=unlabeled_loader,\n",
        "        optimizer=optimizer,\n",
        "        epochs=35,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    torch.save(pretrained_model.state_dict(), 'simclr_resnet15_pretrained.pt')\n",
        "    print(\"SimCLR pretraining completed and model saved\")\n",
        "    encoder = pretrained_model.get_encoder()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEJ_5C5QzJxR",
        "outputId": "84746206-e76f-42df-8a22-7195936feb59"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model initialized\n",
            "Unlabeled dataset size: 60000\n",
            "Number of batches: 468\n",
            "Batch structure verification:\n",
            "  Batch type: <class 'list'>\n",
            "  Views type: <class 'list'>, Labels shape: torch.Size([128])\n",
            "  Views[0] is a list with 2 elements\n",
            "  First view shape: torch.Size([128, 3, 224, 224])\n",
            "  Second view shape: torch.Size([128, 3, 224, 224])\n",
            "Batch structure looks correct for SimCLR training\n",
            "Starting SimCLR pretraining...\n",
            "Epoch [1/35], Learning Rate: 0.000010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-aa1c4f14eedd>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-42-aa1c4f14eedd>:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/35], Loss: 1.4944\n",
            "Epoch [2/35], Learning Rate: 0.001008\n",
            "Epoch [2/35], Loss: 1.4914\n",
            "Epoch [3/35], Learning Rate: 0.002006\n",
            "Epoch [3/35], Loss: 1.4914\n",
            "Epoch [4/35], Learning Rate: 0.003004\n",
            "Epoch [4/35], Loss: 1.4914\n",
            "Epoch [5/35], Learning Rate: 0.004002\n",
            "Epoch [5/35], Loss: 1.4914\n",
            "Epoch [6/35], Learning Rate: 0.005000\n",
            "Epoch [6/35], Loss: 1.4917\n",
            "Epoch [7/35], Learning Rate: 0.004987\n",
            "Epoch [7/35], Loss: 1.4919\n",
            "Epoch [8/35], Learning Rate: 0.004946\n",
            "Epoch [8/35], Loss: 1.4921\n",
            "Epoch [9/35], Learning Rate: 0.004880\n",
            "Epoch [9/35], Loss: 1.4920\n",
            "Epoch [10/35], Learning Rate: 0.004788\n",
            "Epoch [10/35], Loss: 1.4920\n",
            "Epoch [11/35], Learning Rate: 0.004672\n",
            "Epoch [11/35], Loss: 1.4920\n",
            "Epoch [12/35], Learning Rate: 0.004532\n",
            "Epoch [12/35], Loss: 1.4921\n",
            "Epoch [13/35], Learning Rate: 0.004371\n",
            "Epoch [13/35], Loss: 1.4917\n",
            "Epoch [14/35], Learning Rate: 0.004189\n",
            "Epoch [14/35], Loss: 1.4917\n",
            "Epoch [15/35], Learning Rate: 0.003990\n",
            "Epoch [15/35], Loss: 1.4916\n",
            "Epoch [16/35], Learning Rate: 0.003775\n",
            "Epoch [16/35], Loss: 1.4916\n",
            "Epoch [17/35], Learning Rate: 0.003547\n",
            "Epoch [17/35], Loss: 1.4914\n",
            "Epoch [18/35], Learning Rate: 0.003307\n",
            "Epoch [18/35], Loss: 1.4915\n",
            "Epoch [19/35], Learning Rate: 0.003059\n",
            "Epoch [19/35], Loss: 1.4914\n",
            "Epoch [20/35], Learning Rate: 0.002806\n",
            "Epoch [20/35], Loss: 1.4914\n",
            "Epoch [21/35], Learning Rate: 0.002550\n",
            "Epoch [21/35], Loss: 1.4916\n",
            "Epoch [22/35], Learning Rate: 0.002294\n",
            "Epoch [22/35], Loss: 1.4915\n",
            "Epoch [23/35], Learning Rate: 0.002041\n",
            "Epoch [23/35], Loss: 1.4916\n",
            "Epoch [24/35], Learning Rate: 0.001793\n",
            "Epoch [24/35], Loss: 1.4916\n",
            "Epoch [25/35], Learning Rate: 0.001553\n",
            "Epoch [25/35], Loss: 1.4916\n",
            "Epoch [26/35], Learning Rate: 0.001325\n",
            "Epoch [26/35], Loss: 1.4916\n",
            "Epoch [27/35], Learning Rate: 0.001110\n",
            "Epoch [27/35], Loss: 1.4916\n",
            "Epoch [28/35], Learning Rate: 0.000911\n",
            "Epoch [28/35], Loss: 1.4916\n",
            "Epoch [29/35], Learning Rate: 0.000729\n",
            "Epoch [29/35], Loss: 1.4917\n",
            "Epoch [30/35], Learning Rate: 0.000568\n",
            "Epoch [30/35], Loss: 1.4916\n",
            "Epoch [31/35], Learning Rate: 0.000428\n",
            "Epoch [31/35], Loss: 1.4916\n",
            "Epoch [32/35], Learning Rate: 0.000312\n",
            "Epoch [32/35], Loss: 1.4917\n",
            "Epoch [33/35], Learning Rate: 0.000220\n",
            "Epoch [33/35], Loss: 1.4916\n",
            "Epoch [34/35], Learning Rate: 0.000154\n",
            "Epoch [34/35], Loss: 1.4917\n",
            "Epoch [35/35], Learning Rate: 0.000113\n",
            "Epoch [35/35], Loss: 1.4917\n",
            "SimCLR pretraining completed and model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "pySC6koUWFfl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cernbox.cern.ch/remote.php/dav/public-files/e3pqxcIznqdYyRv/Dataset_Specific_labelled.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz7lYOUh76Ic",
        "outputId": "366f54fb-5371-4d62-d931-86af493fea60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-30 21:02:19--  https://cernbox.cern.ch/remote.php/dav/public-files/e3pqxcIznqdYyRv/Dataset_Specific_labelled.h5\n",
            "Resolving cernbox.cern.ch (cernbox.cern.ch)... 137.138.120.151, 128.142.170.17, 128.142.53.28, ...\n",
            "Connecting to cernbox.cern.ch (cernbox.cern.ch)|137.138.120.151|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5000042048 (4.7G) [application/octet-stream]\n",
            "Saving to: ‘Dataset_Specific_labelled.h5’\n",
            "\n",
            "Dataset_Specific_la 100%[===================>]   4.66G  17.4MB/s    in 4m 55s  \n",
            "\n",
            "2025-03-30 21:07:16 (16.2 MB/s) - ‘Dataset_Specific_labelled.h5’ saved [5000042048/5000042048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = torch.load('/content/simclr_resnet15_pretrained.pt')"
      ],
      "metadata": {
        "id": "6pT3MGczgS8P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "def load_pretrained_encoder(path='simclr_resnet15_pretrained.pt', device='cuda'):\n",
        "\n",
        "    encoder = Resnet18Backbone()\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    encoder.load_state_dict(state_dict)\n",
        "    return encoder"
      ],
      "metadata": {
        "id": "vxdk4NJ6FMYs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, criterion_cls, criterion_reg, num_epochs=35, device='cuda'):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y_cls, y_reg in train_loader:\n",
        "            x, y_cls, y_reg = x.to(device), y_cls.to(device), y_reg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            cls_pred, reg_pred = model(x)\n",
        "\n",
        "            loss_cls = criterion_cls(cls_pred, y_cls)\n",
        "            loss_reg = criterion_reg(reg_pred.squeeze(), y_reg)\n",
        "\n",
        "            loss = loss_cls + loss_reg\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "IEWdJnkCGO6X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class JetDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None):\n",
        "        self.file_path = file_path\n",
        "        self.transform = transform\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            self.length = len(f['jet'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.file_path, 'r') as f:\n",
        "\n",
        "            jet_img = f['jet'][idx]\n",
        "            label = f['Y'][idx][0]\n",
        "\n",
        "        if jet_img.max() > 1.0:\n",
        "            jet_img = jet_img / 255.0\n",
        "\n",
        "        jet_img = torch.tensor(jet_img, dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "        if self.transform:\n",
        "            jet_img = self.transform(jet_img)\n",
        "\n",
        "        cls_label = torch.tensor(int(label), dtype=torch.long)\n",
        "        reg_label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return jet_img, cls_label, reg_label\n"
      ],
      "metadata": {
        "id": "taE3_1suGaYj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "def train_classification(model, train_loader, val_loader, num_epochs=35, device='cuda'):\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.encoder.parameters(), 'lr': 1e-4},\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "    ])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"=== Phase 1: Training Classification Head Only ===\")\n",
        "    for param in model.encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=1e-3)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "    for epoch in range(20):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, cls_label, _ in train_loader:\n",
        "            x = x.to(device)\n",
        "            cls_label = cls_label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, cls_label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Phase 1 - Epoch [{epoch+1}/20], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            evaluate_classification(model, val_loader, device)\n",
        "\n",
        "    print(\"\\n=== Phase 2: Fine-tuning Entire Model ===\")\n",
        "    for param in model.encoder.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.encoder.parameters(), 'lr': 5e-5},\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "    ])\n",
        "\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, cls_label, _ in train_loader:\n",
        "            x = x.to(device)\n",
        "            cls_label = cls_label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, cls_label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Phase 2 - Epoch [{epoch+1}/30], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            evaluate_classification(model, val_loader, device)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_regression(model, train_loader, val_loader, num_epochs=35, device='cuda'):\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.encoder.parameters(), 'lr': 1e-4},\n",
        "        {'params': model.regressor.parameters(), 'lr': 1e-3}\n",
        "    ])\n",
        "\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for param in model.encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    head_optimizer = optim.Adam(model.regressor.parameters(), lr=1e-3)\n",
        "    head_scheduler = CosineAnnealingLR(head_optimizer, T_max=15)\n",
        "\n",
        "    for epoch in range(15):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, _, reg_label in train_loader:\n",
        "            x = x.to(device)\n",
        "            reg_label = reg_label.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred.squeeze(), reg_label)\n",
        "\n",
        "            head_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            head_optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        head_scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Phase 1 - Epoch [{epoch+1}/15], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            val_loss = evaluate_regression(model, val_loader, device)\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), 'best_regression_model_phase1.pt')\n",
        "\n",
        "\n",
        "    print(\"\\nPhase 2: Fine-tuning Entire Model\")\n",
        "    for param in model.encoder.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    full_optimizer = optim.Adam([\n",
        "        {'params': model.encoder.parameters(), 'lr': 5e-5},\n",
        "        {'params': model.regressor.parameters(), 'lr': 1e-3}\n",
        "    ], weight_decay=1e-5)\n",
        "\n",
        "    full_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        full_optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "    for epoch in range(35):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, _, reg_label in train_loader:\n",
        "            x = x.to(device)\n",
        "            reg_label = reg_label.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred.squeeze(), reg_label)\n",
        "\n",
        "            full_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            full_optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        full_scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Phase 2 - Epoch [{epoch+1}/35], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            val_loss = evaluate_regression(model, val_loader, device)\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), 'best_regression_model.pt')\n",
        "\n",
        "    model.load_state_dict(torch.load('best_regression_model.pt'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_classification(model, test_loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, cls_label, _ in test_loader:\n",
        "            x = x.to(device)\n",
        "            cls_label = cls_label.to(device)\n",
        "\n",
        "            outputs = model(x)\n",
        "            if isinstance(outputs, tuple) or isinstance(outputs, list):\n",
        "                cls_outputs = outputs[0]  # Assume first output is classification\n",
        "            else:\n",
        "                cls_outputs = outputs  # Model returns only classification\n",
        "\n",
        "            _, predicted = torch.max(cls_outputs.data, 1)\n",
        "            total += cls_label.size(0)\n",
        "            correct += (predicted == cls_label).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Classification Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def evaluate_regression(model, test_loader, device='cuda'):\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _, reg_label in test_loader:\n",
        "            x = x.to(device)\n",
        "            reg_label = reg_label.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred.squeeze(), reg_label)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f\"Regression MSE: {avg_loss:.4f}\")\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "_A-VBJ1mLI1T"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained_encoder(path='simclr_resnet15_pretrained.pt', device='cuda'):\n",
        "    encoder = Resnet18Backbone()\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "\n",
        "    new_state_dict = {}\n",
        "    for key, value in state_dict.items():\n",
        "        if key.startswith(\"encoder.\"):\n",
        "            new_key = key[8:]\n",
        "            new_state_dict[new_key] = value\n",
        "        else:\n",
        "            new_state_dict[key] = value\n",
        "\n",
        "    encoder.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    print(\"Pretrained encoder loaded successfully\")\n",
        "    return encoder\n",
        "\n",
        "    return encoder"
      ],
      "metadata": {
        "id": "SlnFeRgMMUQh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(train_metrics, val_metrics, title=\"Metrics\", ylabel=\"Value\", xlabel=\"Epochs\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_metrics, label=\"Train\")\n",
        "    plt.plot(val_metrics, label=\"Validation\")\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(train_acc_cls, val_acc_cls, title=\"Classification Accuracy\", ylabel=\"Accuracy (%)\")\n",
        "\n",
        "# For regression MSE\n",
        "plot_metrics(train_mse_reg, val_mse_reg, title=\"Regression MSE\", ylabel=\"MSE\")"
      ],
      "metadata": {
        "id": "57HezTEGp8gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "    dataset = JetDataset('Dataset_Specific_labelled.h5')\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "    pretrained_encoder = load_pretrained_encoder()\n",
        "    pretrained_encoder = adapt_encoder_for_8_channels(pretrained_encoder)\n",
        "\n",
        "    pretrained_encoder=pretrained_encoder.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      dummy_input = torch.randn(1, 8, 125, 125).to(device)\n",
        "      dummy_output = pretrained_encoder(dummy_input)\n",
        "      feature_dim = dummy_output.shape[1]\n",
        "      print(f\"Encoder output feature dimension: {feature_dim}\")\n",
        "\n",
        "    enhanced_cls_model = ClassificationModel(\n",
        "        pretrained_encoder,\n",
        "        num_classes=2,\n",
        "        feature_dim=128\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # cls_model = train_classification(cls_model, train_loader, val_loader, num_epochs=150, device=device)\n",
        "\n",
        "    enhanced_cls_model = train_classification(\n",
        "        enhanced_cls_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device=device\n",
        "    )\n",
        "    torch.save(enhanced_cls_model.state_dict(), 'classification_model.pt')\n",
        "\n",
        "    # reg_model = RegressionModel(\n",
        "    #     encoder=pretrained_encoder,\n",
        "    #     output_dim=1,\n",
        "    #     feature_dim=feature_dim\n",
        "    # ).to(device)\n",
        "\n",
        "    # reg_model = train_regression(\n",
        "    #     reg_model,\n",
        "    #     train_loader,\n",
        "    #     val_loader,\n",
        "    #     num_epochs=50,\n",
        "    #     device=device)\n",
        "    # torch.save(reg_model.state_dict(), 'regression_model.pt')\n",
        "\n",
        "\n",
        "    # reg_model = train_regression(reg_model, train_loader, val_loader, num_epochs=150, device=device)\n",
        "\n",
        "    # print(\"Pretrained Models:\")\n",
        "    # cls_acc = evaluate_classification(cls_model, val_loader, device)\n",
        "    # reg_loss = evaluate_regression(reg_model, val_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkNnBdlbGSCG",
        "outputId": "527491b8-c4d8-455a-8a94-2e18e797996c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "Pretrained encoder loaded successfully\n",
            "Encoder output feature dimension: 1000\n",
            "=== Phase 1: Training Classification Head Only ===\n",
            "Phase 1 - Epoch [1/20], Loss: 0.7113\n",
            "Phase 1 - Epoch [2/20], Loss: 0.6980\n",
            "Phase 1 - Epoch [3/20], Loss: 0.6968\n",
            "Phase 1 - Epoch [4/20], Loss: 0.6927\n",
            "Phase 1 - Epoch [5/20], Loss: 0.6911\n",
            "Classification Accuracy: 52.10%\n",
            "Phase 1 - Epoch [6/20], Loss: 0.6895\n",
            "Phase 1 - Epoch [7/20], Loss: 0.6903\n",
            "Phase 1 - Epoch [8/20], Loss: 0.6900\n",
            "Phase 1 - Epoch [9/20], Loss: 0.6888\n",
            "Phase 1 - Epoch [10/20], Loss: 0.6865\n",
            "Classification Accuracy: 53.20%\n",
            "Phase 1 - Epoch [11/20], Loss: 0.6862\n",
            "Phase 1 - Epoch [12/20], Loss: 0.6853\n",
            "Phase 1 - Epoch [13/20], Loss: 0.6848\n",
            "Phase 1 - Epoch [14/20], Loss: 0.6845\n",
            "Phase 1 - Epoch [15/20], Loss: 0.6836\n",
            "Classification Accuracy: 55.80%\n",
            "Phase 1 - Epoch [16/20], Loss: 0.6840\n",
            "Phase 1 - Epoch [17/20], Loss: 0.6833\n",
            "Phase 1 - Epoch [18/20], Loss: 0.6818\n",
            "Phase 1 - Epoch [19/20], Loss: 0.6816\n",
            "Phase 1 - Epoch [20/20], Loss: 0.6825\n",
            "Classification Accuracy: 55.20%\n",
            "\n",
            "=== Phase 2: Fine-tuning Entire Model ===\n",
            "Phase 2 - Epoch [1/30], Loss: 0.6264\n",
            "Phase 2 - Epoch [2/30], Loss: 0.4397\n",
            "Phase 2 - Epoch [3/30], Loss: 0.3710\n",
            "Phase 2 - Epoch [4/30], Loss: 0.3388\n",
            "Phase 2 - Epoch [5/30], Loss: 0.3222\n",
            "Classification Accuracy: 88.75%\n",
            "Phase 2 - Epoch [6/30], Loss: 0.3096\n",
            "Phase 2 - Epoch [7/30], Loss: 0.3057\n",
            "Phase 2 - Epoch [8/30], Loss: 0.2980\n",
            "Phase 2 - Epoch [9/30], Loss: 0.2939\n",
            "Phase 2 - Epoch [10/30], Loss: 0.2897\n",
            "Classification Accuracy: 89.55%\n",
            "Phase 2 - Epoch [11/30], Loss: 0.3033\n",
            "Phase 2 - Epoch [12/30], Loss: 0.3004\n",
            "Phase 2 - Epoch [13/30], Loss: 0.2906\n",
            "Phase 2 - Epoch [14/30], Loss: 0.2841\n",
            "Phase 2 - Epoch [15/30], Loss: 0.2689\n",
            "Classification Accuracy: 79.85%\n",
            "Phase 2 - Epoch [16/30], Loss: 0.2622\n",
            "Phase 2 - Epoch [17/30], Loss: 0.2488\n",
            "Phase 2 - Epoch [18/30], Loss: 0.2323\n",
            "Phase 2 - Epoch [19/30], Loss: 0.2261\n",
            "Phase 2 - Epoch [20/30], Loss: 0.2131\n",
            "Classification Accuracy: 84.50%\n",
            "Phase 2 - Epoch [21/30], Loss: 0.2005\n",
            "Phase 2 - Epoch [22/30], Loss: 0.1799\n",
            "Phase 2 - Epoch [23/30], Loss: 0.1750\n",
            "Phase 2 - Epoch [24/30], Loss: 0.1653\n",
            "Phase 2 - Epoch [25/30], Loss: 0.1524\n",
            "Classification Accuracy: 88.05%\n",
            "Phase 2 - Epoch [26/30], Loss: 0.1473\n",
            "Phase 2 - Epoch [27/30], Loss: 0.1438\n",
            "Phase 2 - Epoch [28/30], Loss: 0.1364\n",
            "Phase 2 - Epoch [29/30], Loss: 0.1374\n",
            "Phase 2 - Epoch [30/30], Loss: 0.1324\n",
            "Classification Accuracy: 87.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "    dataset = JetDataset('Dataset_Specific_labelled.h5')\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "    pretrained_encoder = load_pretrained_encoder()\n",
        "    pretrained_encoder = adapt_encoder_for_8_channels(pretrained_encoder)\n",
        "\n",
        "    pretrained_encoder=pretrained_encoder.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      dummy_input = torch.randn(1, 8, 125, 125).to(device)\n",
        "      dummy_output = pretrained_encoder(dummy_input)\n",
        "      feature_dim = dummy_output.shape[1]\n",
        "      print(f\"Encoder output feature dimension: {feature_dim}\")\n",
        "\n",
        "    reg_model = RegressionModel(\n",
        "        encoder=pretrained_encoder,\n",
        "        output_dim=1,\n",
        "        feature_dim=feature_dim\n",
        "    ).to(device)\n",
        "\n",
        "    reg_model = train_regression(\n",
        "        reg_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_epochs=50,\n",
        "        device=device)\n",
        "    torch.save(reg_model.state_dict(), 'regression_model.pt')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NwJX2-jvRgO",
        "outputId": "d588b03a-4ec3-40cd-890b-a1af27b3136a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "Pretrained encoder loaded successfully\n",
            "Encoder output feature dimension: 1000\n",
            "Phase 1 - Epoch [1/15], Loss: 0.2925\n",
            "Phase 1 - Epoch [2/15], Loss: 0.2604\n",
            "Phase 1 - Epoch [3/15], Loss: 0.2572\n",
            "Phase 1 - Epoch [4/15], Loss: 0.2579\n",
            "Phase 1 - Epoch [5/15], Loss: 0.2535\n",
            "Regression MSE: 0.2538\n",
            "Phase 1 - Epoch [6/15], Loss: 0.2513\n",
            "Phase 1 - Epoch [7/15], Loss: 0.2494\n",
            "Phase 1 - Epoch [8/15], Loss: 0.2522\n",
            "Phase 1 - Epoch [9/15], Loss: 0.2497\n",
            "Phase 1 - Epoch [10/15], Loss: 0.2490\n",
            "Regression MSE: 0.2494\n",
            "Phase 1 - Epoch [11/15], Loss: 0.2482\n",
            "Phase 1 - Epoch [12/15], Loss: 0.2479\n",
            "Phase 1 - Epoch [13/15], Loss: 0.2472\n",
            "Phase 1 - Epoch [14/15], Loss: 0.2465\n",
            "Phase 1 - Epoch [15/15], Loss: 0.2464\n",
            "Regression MSE: 0.2465\n",
            "\n",
            "Phase 2: Fine-tuning Entire Model\n",
            "Phase 2 - Epoch [1/35], Loss: 0.2510\n",
            "Phase 2 - Epoch [2/35], Loss: 0.1867\n",
            "Phase 2 - Epoch [3/35], Loss: 0.1197\n",
            "Phase 2 - Epoch [4/35], Loss: 0.1030\n",
            "Phase 2 - Epoch [5/35], Loss: 0.0959\n",
            "Regression MSE: 0.1192\n",
            "Phase 2 - Epoch [6/35], Loss: 0.0932\n",
            "Phase 2 - Epoch [7/35], Loss: 0.0893\n",
            "Phase 2 - Epoch [8/35], Loss: 0.0868\n",
            "Phase 2 - Epoch [9/35], Loss: 0.0842\n",
            "Phase 2 - Epoch [10/35], Loss: 0.0848\n",
            "Regression MSE: 0.0920\n",
            "Phase 2 - Epoch [11/35], Loss: 0.0905\n",
            "Phase 2 - Epoch [12/35], Loss: 0.0902\n",
            "Phase 2 - Epoch [13/35], Loss: 0.0873\n",
            "Phase 2 - Epoch [14/35], Loss: 0.0835\n",
            "Phase 2 - Epoch [15/35], Loss: 0.0798\n",
            "Regression MSE: 0.1189\n",
            "Phase 2 - Epoch [16/35], Loss: 0.0753\n",
            "Phase 2 - Epoch [17/35], Loss: 0.0716\n",
            "Phase 2 - Epoch [18/35], Loss: 0.0670\n",
            "Phase 2 - Epoch [19/35], Loss: 0.0625\n",
            "Phase 2 - Epoch [20/35], Loss: 0.0623\n",
            "Regression MSE: 0.1112\n",
            "Phase 2 - Epoch [21/35], Loss: 0.0568\n",
            "Phase 2 - Epoch [22/35], Loss: 0.0539\n",
            "Phase 2 - Epoch [23/35], Loss: 0.0510\n",
            "Phase 2 - Epoch [24/35], Loss: 0.0479\n",
            "Phase 2 - Epoch [25/35], Loss: 0.0457\n",
            "Regression MSE: 0.1104\n",
            "Phase 2 - Epoch [26/35], Loss: 0.0444\n",
            "Phase 2 - Epoch [27/35], Loss: 0.0417\n",
            "Phase 2 - Epoch [28/35], Loss: 0.0416\n",
            "Phase 2 - Epoch [29/35], Loss: 0.0434\n",
            "Phase 2 - Epoch [30/35], Loss: 0.0421\n",
            "Regression MSE: 0.1093\n",
            "Phase 2 - Epoch [31/35], Loss: 0.0551\n",
            "Phase 2 - Epoch [32/35], Loss: 0.0524\n",
            "Phase 2 - Epoch [33/35], Loss: 0.0502\n",
            "Phase 2 - Epoch [34/35], Loss: 0.0491\n",
            "Phase 2 - Epoch [35/35], Loss: 0.0484\n",
            "Regression MSE: 0.1695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adapt_encoder_for_8_channels(encoder, device='cuda'):\n",
        "\n",
        "    first_conv = encoder.conv1\n",
        "\n",
        "    original_weights = first_conv.weight.data\n",
        "\n",
        "    new_conv = nn.Conv2d(\n",
        "        in_channels=8,\n",
        "        out_channels=first_conv.out_channels,\n",
        "        kernel_size=first_conv.kernel_size,\n",
        "        stride=first_conv.stride,\n",
        "        padding=first_conv.padding,\n",
        "        bias=first_conv.bias is not None\n",
        "    ).to(device)\n",
        "\n",
        "    new_conv.weight.data[:, :3, :, :] = original_weights\n",
        "    new_conv.weight.data[:, 3:, :, :].normal_(0, 0.01)\n",
        "\n",
        "\n",
        "    if first_conv.bias is not None:\n",
        "        new_conv.bias.data = first_conv.bias.data.clone()\n",
        "\n",
        "\n",
        "    encoder.conv1 = new_conv\n",
        "\n",
        "    return encoder\n"
      ],
      "metadata": {
        "id": "42eczsZhNLeG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device='cuda'):\n",
        "    model.eval()\n",
        "    cls_criterion = nn.CrossEntropyLoss()\n",
        "    reg_criterion = nn.MSELoss()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    cls_loss_sum = 0.0\n",
        "    reg_loss_sum = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, cls_labels, reg_labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            cls_labels = cls_labels.to(device)\n",
        "            reg_labels = reg_labels.to(device)\n",
        "\n",
        "            cls_outputs, reg_outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(cls_outputs.data, 1)\n",
        "            total += cls_labels.size(0)\n",
        "            correct += (predicted == cls_labels).sum().item()\n",
        "\n",
        "            cls_loss = cls_criterion(cls_outputs, cls_labels)\n",
        "            reg_loss = reg_criterion(reg_outputs.squeeze(), reg_labels)\n",
        "\n",
        "            cls_loss_sum += cls_loss.item()\n",
        "            reg_loss_sum += reg_loss.item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_cls_loss = cls_loss_sum / len(test_loader)\n",
        "    avg_reg_loss = reg_loss_sum / len(test_loader)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Cls Loss: {avg_cls_loss:.4f}, Reg Loss: {avg_reg_loss:.4f}\")\n",
        "    return accuracy, avg_cls_loss, avg_reg_loss\n"
      ],
      "metadata": {
        "id": "syy1i3xkHcym"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchModel(nn.Module):\n",
        "    def __init__(self, input_channels=8, num_classes=2):\n",
        "        super(ScratchModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "        self.regressor = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        cls_output = self.classifier(features)\n",
        "        reg_output = self.regressor(features)\n",
        "        return cls_output, reg_output\n"
      ],
      "metadata": {
        "id": "SVSmGm2Eic2-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_scratch_model(model, train_loader, val_loader, task_type, num_epochs=50, device='cuda'):\n",
        "    if task_type == 'classification':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, cls_label, reg_label in train_loader:\n",
        "            x = x.to(device)\n",
        "\n",
        "            if task_type == 'classification':\n",
        "                target = cls_label.to(device)\n",
        "                output = model(x)[0]\n",
        "            else:\n",
        "                target = reg_label.to(device)\n",
        "                output = model(x)[1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(output.squeeze(), target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            if task_type == 'classification':\n",
        "                evaluate_classification(model, val_loader, device)\n",
        "            else:\n",
        "                evaluate_regression(model, val_loader, device)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4ySXEh90if-A"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(model, test_loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, cls_label, _ in test_loader:\n",
        "            x = x.to(device)\n",
        "            cls_label = cls_label.to(device)\n",
        "\n",
        "            outputs, _ = model(x)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += cls_label.size(0)\n",
        "            correct += (predicted == cls_label).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Classification Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_regression(model, test_loader, device='cuda'):\n",
        "\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _, reg_label in test_loader:\n",
        "            x = x.to(device)\n",
        "            reg_label = reg_label.to(device)\n",
        "\n",
        "            _, pred = model(x)\n",
        "            loss = criterion(pred.squeeze(), reg_label)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f\"Regression MSE: {avg_loss:.4f}\")\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "-HnFIipCijRv"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "    dataset = JetDataset('Dataset_Specific_labelled.h5')\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "    scratch_cls_model = ScratchModel(input_channels=8, num_classes=2).to(device)\n",
        "    scratch_cls_model = train_scratch_model(\n",
        "        scratch_cls_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        task_type='classification',\n",
        "        num_epochs=50,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    scratch_reg_model = ScratchModel(input_channels=8, num_classes=2).to(device)\n",
        "    scratch_reg_model = train_scratch_model(\n",
        "        scratch_reg_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        task_type='regression',\n",
        "        num_epochs=50,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    torch.save(scratch_cls_model.state_dict(), 'scratch_classification_model.pt')\n",
        "    torch.save(scratch_reg_model.state_dict(), 'scratch_regression_model.pt')\n",
        "    print(\"Classification model from scratch:\")\n",
        "    cls_acc = evaluate_classification(scratch_cls_model, val_loader, device)\n",
        "\n",
        "    print(\"Regression model from scratch:\")\n",
        "    reg_mse = evaluate_regression(scratch_reg_model, val_loader, device)\n",
        "\n",
        "    print(f\"Classification Accuracy: {cls_acc:.2f}%\")\n",
        "    print(f\"Regression MSE: {reg_mse:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF9FGxYRipNt",
        "outputId": "851ce5d4-62ed-410e-987b-1d5f5815ceea"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "Epoch [1/50], Loss: 0.4499\n",
            "Epoch [2/50], Loss: 0.3050\n",
            "Epoch [3/50], Loss: 0.2519\n",
            "Epoch [4/50], Loss: 0.1960\n",
            "Epoch [5/50], Loss: 0.1399\n",
            "Classification Accuracy: 86.20%\n",
            "Epoch [6/50], Loss: 0.0951\n",
            "Epoch [7/50], Loss: 0.0674\n",
            "Epoch [8/50], Loss: 0.0435\n",
            "Epoch [9/50], Loss: 0.0404\n",
            "Epoch [10/50], Loss: 0.0303\n",
            "Classification Accuracy: 77.75%\n",
            "Epoch [11/50], Loss: 0.0132\n",
            "Epoch [12/50], Loss: 0.0099\n",
            "Epoch [13/50], Loss: 0.0085\n",
            "Epoch [14/50], Loss: 0.0173\n",
            "Epoch [15/50], Loss: 0.0226\n",
            "Classification Accuracy: 83.45%\n",
            "Epoch [16/50], Loss: 0.0266\n",
            "Epoch [17/50], Loss: 0.0143\n",
            "Epoch [18/50], Loss: 0.0060\n",
            "Epoch [19/50], Loss: 0.0046\n",
            "Epoch [20/50], Loss: 0.0031\n",
            "Classification Accuracy: 84.65%\n",
            "Epoch [21/50], Loss: 0.0027\n",
            "Epoch [22/50], Loss: 0.0019\n",
            "Epoch [23/50], Loss: 0.0011\n",
            "Epoch [24/50], Loss: 0.0016\n",
            "Epoch [25/50], Loss: 0.0004\n",
            "Classification Accuracy: 85.00%\n",
            "Epoch [26/50], Loss: 0.0004\n",
            "Epoch [27/50], Loss: 0.0003\n",
            "Epoch [28/50], Loss: 0.0002\n",
            "Epoch [29/50], Loss: 0.0001\n",
            "Epoch [30/50], Loss: 0.0001\n",
            "Classification Accuracy: 84.80%\n",
            "Epoch [31/50], Loss: 0.0002\n",
            "Epoch [32/50], Loss: 0.0002\n",
            "Epoch [33/50], Loss: 0.0002\n",
            "Epoch [34/50], Loss: 0.0002\n",
            "Epoch [35/50], Loss: 0.0002\n",
            "Classification Accuracy: 85.15%\n",
            "Epoch [36/50], Loss: 0.0002\n",
            "Epoch [37/50], Loss: 0.0001\n",
            "Epoch [38/50], Loss: 0.0002\n",
            "Epoch [39/50], Loss: 0.0004\n",
            "Epoch [40/50], Loss: 0.0001\n",
            "Classification Accuracy: 85.05%\n",
            "Epoch [41/50], Loss: 0.0002\n",
            "Epoch [42/50], Loss: 0.0001\n",
            "Epoch [43/50], Loss: 0.0001\n",
            "Epoch [44/50], Loss: 0.0001\n",
            "Epoch [45/50], Loss: 0.0001\n",
            "Classification Accuracy: 84.60%\n",
            "Epoch [46/50], Loss: 0.0001\n",
            "Epoch [47/50], Loss: 0.0001\n",
            "Epoch [48/50], Loss: 0.0002\n",
            "Epoch [49/50], Loss: 0.0001\n",
            "Epoch [50/50], Loss: 0.0002\n",
            "Classification Accuracy: 85.10%\n",
            "Epoch [1/50], Loss: 0.1868\n",
            "Epoch [2/50], Loss: 0.1098\n",
            "Epoch [3/50], Loss: 0.0962\n",
            "Epoch [4/50], Loss: 0.0822\n",
            "Epoch [5/50], Loss: 0.0719\n",
            "Regression MSE: 0.1375\n",
            "Epoch [6/50], Loss: 0.0605\n",
            "Epoch [7/50], Loss: 0.0512\n",
            "Epoch [8/50], Loss: 0.0404\n",
            "Epoch [9/50], Loss: 0.0324\n",
            "Epoch [10/50], Loss: 0.0242\n",
            "Regression MSE: 0.1242\n",
            "Epoch [11/50], Loss: 0.0213\n",
            "Epoch [12/50], Loss: 0.0195\n",
            "Epoch [13/50], Loss: 0.0175\n",
            "Epoch [14/50], Loss: 0.0167\n",
            "Epoch [15/50], Loss: 0.0142\n",
            "Regression MSE: 0.1840\n",
            "Epoch [16/50], Loss: 0.0130\n",
            "Epoch [17/50], Loss: 0.0119\n",
            "Epoch [18/50], Loss: 0.0120\n",
            "Epoch [19/50], Loss: 0.0098\n",
            "Epoch [20/50], Loss: 0.0106\n",
            "Regression MSE: 0.1355\n",
            "Epoch [21/50], Loss: 0.0097\n",
            "Epoch [22/50], Loss: 0.0066\n",
            "Epoch [23/50], Loss: 0.0067\n",
            "Epoch [24/50], Loss: 0.0073\n",
            "Epoch [25/50], Loss: 0.0065\n",
            "Regression MSE: 0.1791\n",
            "Epoch [26/50], Loss: 0.0060\n",
            "Epoch [27/50], Loss: 0.0055\n",
            "Epoch [28/50], Loss: 0.0049\n",
            "Epoch [29/50], Loss: 0.0044\n",
            "Epoch [30/50], Loss: 0.0041\n",
            "Regression MSE: 0.1385\n",
            "Epoch [31/50], Loss: 0.0044\n",
            "Epoch [32/50], Loss: 0.0038\n",
            "Epoch [33/50], Loss: 0.0032\n",
            "Epoch [34/50], Loss: 0.0035\n",
            "Epoch [35/50], Loss: 0.0033\n",
            "Regression MSE: 0.1449\n",
            "Epoch [36/50], Loss: 0.0036\n",
            "Epoch [37/50], Loss: 0.0034\n",
            "Epoch [38/50], Loss: 0.0035\n",
            "Epoch [39/50], Loss: 0.0033\n",
            "Epoch [40/50], Loss: 0.0028\n",
            "Regression MSE: 0.1290\n",
            "Epoch [41/50], Loss: 0.0027\n",
            "Epoch [42/50], Loss: 0.0019\n",
            "Epoch [43/50], Loss: 0.0026\n",
            "Epoch [44/50], Loss: 0.0020\n",
            "Epoch [45/50], Loss: 0.0029\n",
            "Regression MSE: 0.1276\n",
            "Epoch [46/50], Loss: 0.0024\n",
            "Epoch [47/50], Loss: 0.0018\n",
            "Epoch [48/50], Loss: 0.0020\n",
            "Epoch [49/50], Loss: 0.0020\n",
            "Epoch [50/50], Loss: 0.0017\n",
            "Regression MSE: 0.1287\n",
            "Classification model from scratch:\n",
            "Classification Accuracy: 85.10%\n",
            "Regression model from scratch:\n",
            "Regression MSE: 0.1287\n",
            "Classification Accuracy: 85.10%\n",
            "Regression MSE: 0.1287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ELXso5CipnI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}